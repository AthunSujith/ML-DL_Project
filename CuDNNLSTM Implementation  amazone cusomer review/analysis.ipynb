{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc829df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import bz2   #for the Read and write .bz2 compressed files\n",
    "import gc   #Manage memory by forcing Pythonâ€™s garbage collector to release unused \n",
    "import chardet #Detect character encoding in text files (e.g., UTF-8, ISO-8859-1).\n",
    "import re #Regular expressions (searching, matching, and manipulating text patterns)\n",
    "import os\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451aa5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_output_length(input_length, filter_size, padding, stride, dilation=1):\n",
    "    if input_length is None:\n",
    "        return None\n",
    "\n",
    "    assert padding in {'same', 'valid', 'full'}\n",
    "    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n",
    "\n",
    "    if padding == 'same':\n",
    "        return (input_length + stride - 1) // stride\n",
    "    elif padding == 'valid':\n",
    "        return (input_length - dilated_filter_size + stride) // stride\n",
    "    elif padding == 'full':\n",
    "        return (input_length + dilated_filter_size - 1) // stride\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "950f4f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe version of conv_output_length\n",
    "def conv_output_length(input_length, filter_size, padding, stride, dilation=1):\n",
    "    if input_length is None:\n",
    "        return None\n",
    "    assert padding in {'same', 'valid', 'full'}\n",
    "    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n",
    "    if padding == 'same':\n",
    "        return (input_length + stride - 1) // stride\n",
    "    elif padding == 'valid':\n",
    "        return (input_length - dilated_filter_size + stride) // stride\n",
    "    elif padding == 'full':\n",
    "        return (input_length + dilated_filter_size - 1) // stride\n",
    "\n",
    "# Keras / TensorFlow imports\n",
    "\n",
    "# Keras / TensorFlow\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input, Conv1D, GlobalMaxPool1D, Dropout, concatenate, Layer, InputSpec, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import activations, initializers, regularizers, constraints\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.constraints import MaxNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23f6b2b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.ft.txt.bz2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_file \u001b[38;5;241m=\u001b[39m \u001b[43mbz2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBZ2File\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain.ft.txt.bz2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m test_file \u001b[38;5;241m=\u001b[39m bz2\u001b[38;5;241m.\u001b[39mBZ2File(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.ft.txt.bz2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\athun\\AppData\\Local\\Programs\\Python\\Python310\\lib\\bz2.py:81\u001b[0m, in \u001b[0;36mBZ2File.__init__\u001b[1;34m(self, filename, mode, compresslevel)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid mode: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (mode,))\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike)):\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp \u001b[38;5;241m=\u001b[39m \u001b[43m_builtin_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closefp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode_code\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.ft.txt.bz2'"
     ]
    }
   ],
   "source": [
    "train_file = bz2.BZ2File('train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f1d2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_file_lines = train_file.readlines()\n",
    "test_file_lines = test_file.readlines()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b53b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "del train_file, test_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a40ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_file_lines = [x.decode('utf-8') for x in train_file_lines]\n",
    "test_file_lines = [x.decode('utf-8') for x in test_file_lines]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758fe18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\n",
    "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines]\n",
    "\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
    "    \n",
    "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]\n",
    "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file_lines]\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n",
    "                                                       \n",
    "for i in range(len(train_sentences)):\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
    "        \n",
    "for i in range(len(test_sentences)):\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0cba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_file_lines, test_file_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d337c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d533f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871676d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2424f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c97af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = tokenizer.texts_to_sequences(train_sentences)\n",
    "X_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c7a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d2f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenized_test = tokenizer.texts_to_sequences(test_sentences)\n",
    "X_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcc6de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "EMBEDDING_FILE = '../input/glovetwitter100d/glove.twitter.27B.100d.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aacc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84108e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "#change below line if computing normal stats is too slow\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size)) #embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608af0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "del tokenized_test, tokenized_train, tokenizer, train_sentences, test_sentences, word_index, embeddings_index, all_embs, nb_words\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8193bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_size = 2048\n",
    "epochs = 7\n",
    "embed_size = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82de448",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556ca5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cudnnlstm_model(conv_layers = 2, max_dilation_rate = 3):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv1D(2*embed_size, kernel_size = 3)(x)\n",
    "    prefilt = Conv1D(2*embed_size, kernel_size = 3)(x)\n",
    "    x = prefilt\n",
    "    for strides in [1, 1, 2]:\n",
    "        x = Conv1D(128*2**(strides), strides = strides, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_size=3, kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)\n",
    "    x_f = CuDNNLSTM(512, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)  \n",
    "    x_b = CuDNNLSTM(512, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)\n",
    "    x = concatenate([x_f, x_b])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['binary_accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "cudnnlstm_model = cudnnlstm_model()\n",
    "cudnnlstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5fc6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path=\"early_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "callbacks = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2d5589",
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnnlstm_model.fit(X_train, train_labels, batch_size=batch_size, epochs=epochs, shuffle = True, validation_split=0.20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cudnnlstm_model.load_weights(weight_path)\n",
    "score, acc = cudnnlstm_model.evaluate(X_test, test_labels, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
